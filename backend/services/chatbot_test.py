import os
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain_weaviate.vectorstores import WeaviateVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from weaviate import WeaviateClient
from weaviate.connect import ConnectionParams

# âœ… ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ .env ë¡œë“œ
# chatbot_test.py â†’ backend/services/chatbot_test.py ê¸°ì¤€
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(ROOT_DIR, ".env"))

# âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
WEAVIATE_HOST = os.getenv("WEAVIATE_HOST", "localhost")
WEAVIATE_PORT = int(os.getenv("WEAVIATE_PORT", "8080"))
WEAVIATE_GRPC_PORT = int(os.getenv("WEAVIATE_GRPC_PORT", "50051"))
WEAVIATE_INDEX_NAME = os.getenv("WEAVIATE_INDEX_NAME", "news_bbc")

# âœ… LLM ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0,
    openai_api_key=OPENAI_API_KEY
)

# âœ… Weaviate ì—°ê²°
connection_params = ConnectionParams.from_params(
    http_host=WEAVIATE_HOST,
    http_port=WEAVIATE_PORT,
    http_secure=False,
    grpc_host=WEAVIATE_HOST,
    grpc_port=WEAVIATE_GRPC_PORT,
    grpc_secure=False
)
client = WeaviateClient(connection_params=connection_params)
client.connect()

# âœ… ë²¡í„°ìŠ¤í† ì–´ ë° QA ì²´ì¸ ì´ˆê¸°í™”
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = WeaviateVectorStore(
    client=client,
    embedding=embedding_model,
    index_name=WEAVIATE_INDEX_NAME,
    text_key="text"
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    memory=memory,
    return_source_documents=True,
    output_key="answer"
)

def clean_text(text: str) -> str:
    """UTF-8 ì¸ì½”ë”© ë¶ˆê°€ëŠ¥í•œ ë¬¸ìë¥¼ ì œê±°í•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜"""
    return text.encode("utf-8", "ignore").decode("utf-8")

def run_qa(question: str):
    """ì§ˆë¬¸ì„ ë°›ì•„ì„œ RAG ê¸°ë°˜ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ì…ë ¥ ì •ë¦¬ í¬í•¨)"""
    question = clean_text(question)
    response = qa_chain.invoke({"question": question})
    
    sources = []
    for doc in response.get("source_documents", []):
        url = doc.metadata.get("url", "ì¶œì²˜ ì—†ìŒ")
        sources.append(clean_text(url))

    return {
        "answer": clean_text(response["answer"]),
        "sources": sources
    }


def close_client():
    client.close()

# CLI ëª¨ë“œ
def chat():
    print("ğŸ—ï¸ ë‰´ìŠ¤ ê¸°ë°˜ RAG ì±—ë´‡ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")
    while True:
        question = input("ğŸ‘¤ ì§ˆë¬¸: ")
        if question.lower() in ("exit", "quit"):
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        result = run_qa(question)
        print(f"\nğŸ¤– ë‹µë³€: {result['answer']}")
        print("ğŸ”— ì¶œì²˜:")
        for src in result['sources']:
            print(f" - {src}")
        print("\n" + "-" * 50 + "\n")

if __name__ == "__main__":
    try:
        chat()
    finally:
        close_client()
