import os
import json
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import openai
import chromadb
from chromadb.config import Settings
from datetime import datetime, timedelta
from langchain_community.document_loaders import JSONLoader

load_dotenv()

# ====================== ì„¤ì • ===========================
NEWS_API_KEY = os.getenv("NEWS_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
USE_OPENAI = True
VECTOR_DB_DIR = "data/vector_db"
NEWS_JSON_FILE = "data/news_articles.json"
hf_model = SentenceTransformer('all-MiniLM-L6-v2')
openai.api_key = OPENAI_API_KEY

# ====================== ë‰´ìŠ¤ ìˆ˜ì§‘ ===========================
def fetch_news(query: str, source: str = "bbc-news"):
    """NewsAPIì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "sources": source,
        "apiKey": NEWS_API_KEY
    }
    response = requests.get(url, params=params)

    if response.status_code != 200:
        print(f"âŒ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨: {response.text}")
        return []

    articles = response.json().get("articles", [])
    results = []
    for article in articles:
        if not article.get("url"):
            continue
        full_content = scrape_full_content(article["url"])
        results.append({
            "title": article.get("title", "No Title"),
            "content": full_content or article.get("content", "No Content"),
            "description": article.get("description", "No Description"),
            "url": article.get("url"),
            "source": article.get("source", {}).get("name", "Unknown Source")
        })
    return results

def scrape_full_content(url: str) -> str:
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            return ' '.join(p.get_text() for p in paragraphs)
    except Exception as e:
        print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url} ({str(e)})")
    return None

# ====================== ì„ë² ë”© ìƒì„± ===========================
def get_embedding(text: str):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° ìƒì„±"""
    if USE_OPENAI:
        response = openai.Embedding.create(
            input=text,
            model="text-embedding-ada-002"
        )
        return response["data"][0]["embedding"]
    else:
        return hf_model.encode(text)

# ====================== ë²¡í„° DB ì €ì¥ ===========================
class VectorDB:
    def __init__(self, collection_name="news_vectors"):
        self.client = chromadb.Client(Settings(persist_directory=VECTOR_DB_DIR))
        self.collection = self.client.get_or_create_collection(collection_name)

    def add_vector(self, vector, metadata):
        existing = self.collection.query(where={"url": metadata["url"]}, n_results=1)
        if existing["ids"]:
            print(f"âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL: {metadata['url']}")
            return
        self.collection.add(embeddings=[vector], metadatas=[metadata])

    def search(self, query_vector, top_k=5):
        return self.collection.query(query_embeddings=[query_vector], n_results=top_k)


# ====================== ë°˜ë³µ ë‰´ìŠ¤ í•„í„°ë§ í•¨ìˆ˜ ===========================
def is_repetitive_bbc_news(title: str, description: str = "") -> bool:
    title = title.strip().lower()
    repetitive_titles = [
        "the latest five minute news bulletin from bbc world service.",
        "news briefing",
        "bbc world news bulletin",
    ]

    repetitive_keywords = [
        "five minute news bulletin",
        "bbc world service",
        "news summary",
        "news update",
        "listen to the latest news",
    ]

    if title in repetitive_titles:
        return True

    for keyword in repetitive_keywords:
        if keyword in title or keyword in description.lower():
            return True

    return False


# ====================== ë©”ì¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ===========================
def fetch_bbc_news_from_to(query: str, start_date: str, end_date: str):
    """
    ë¬´ë£Œ NewsAPI ì¡°ê±´ì— ë§ì¶° BBC ë‰´ìŠ¤ ìˆ˜ì§‘
    - page=1ë§Œ í—ˆìš© (100ê°œê¹Œì§€)
    - ë°˜ë³µì„± ë‰´ìŠ¤ í•„í„°ë§
    """
    url = "https://newsapi.org/v2/everything"
    all_results = []
    api_call_count = 0

    # ë‚ ì§œ íŒŒì‹±
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    delta = timedelta(days=5)  # ë‚ ì§œë¥¼ ë” ì„¸ë¶„í™”í•´ì„œ ì˜ ê°€ì ¸ì˜¤ê¸°

    # ë¬´ë£Œ í”Œëœì—ì„œëŠ” í˜„ì¬ ê¸°ì¤€ 30ì¼ ì´ë‚´ë§Œ ê°€ëŠ¥
    cutoff_date = datetime.utcnow() - timedelta(days=30)
    if start_dt < cutoff_date:
        print(f"âš ï¸ ì‹œì‘ì¼ {start_date}ì€ ë¬´ë£Œ í”Œëœ ë²”ìœ„ë¥¼ ì´ˆê³¼í•¨. {cutoff_date.date()} ì´í›„ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        return []

    while start_dt < end_dt:
        from_str = start_dt.strftime("%Y-%m-%d")
        to_dt = min(start_dt + delta, end_dt)
        to_str = to_dt.strftime("%Y-%m-%d")
        print(f"\nğŸ“… ìˆ˜ì§‘ ì¤‘: {from_str} ~ {to_str}")

        # ë¬´ë£Œ í”Œëœì€ page=1ê¹Œì§€ë§Œ í—ˆìš©
        params = {
            "q": query,
            "sources": "bbc-news",
            "from": from_str,
            "to": to_str,
            "sortBy": "publishedAt",
            "language": "en",
            "pageSize": 100,
            "page": 1,
            "apiKey": NEWS_API_KEY
        }

        response = requests.get(url, params=params)
        api_call_count += 1

        if response.status_code != 200:
            print(f"âŒ API ì‹¤íŒ¨: {response.text}")
            start_dt = to_dt
            continue

        articles = response.json().get("articles", [])
        if not articles:
            print(f"â— ê¸°ì‚¬ ì—†ìŒ (page 1)")
            start_dt = to_dt
            continue

        print(f"ğŸ“„ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")

        repetitive_news_count = 0  # â±ï¸ ë°˜ë³µ ë‰´ìŠ¤ ê°œìˆ˜ ì¹´ìš´í„°
        for article in articles:
            title = article.get("title", "").strip()
            description = article.get("description", "")

            # âœ… ë°˜ë³µì„± ë‰´ìŠ¤ í•„í„°ë§
            if is_repetitive_bbc_news(title, description):
                repetitive_news_count += 1
                continue
            
            full_content = scrape_full_content(article["url"])
            # âœ… ìœ íš¨ ë‰´ìŠ¤ ì €ì¥
            all_results.append({
                "title": article.get("title", "No Title"),
                "content": full_content,
                "description": article.get("description", "No Description"),
                "url": article.get("url"),
                "source": article.get("source", {}).get("name", "Unknown Source")
            })

        # ğŸ”š ë°˜ë³µë¬¸ ëë‚œ í›„ ì¶œë ¥
        print(f"\nğŸ” ì œì™¸ëœ ë°˜ë³µ ë‰´ìŠ¤ ê°œìˆ˜: {repetitive_news_count}ê°œ")
        start_dt = to_dt

    print(f"\nğŸ“¡ ì´ API í˜¸ì¶œ íšŸìˆ˜: {api_call_count}íšŒ")
    print(f"ğŸ“° ìµœì¢… ìˆ˜ì§‘ëœ ìœ íš¨ ê¸°ì‚¬ ìˆ˜: {len(all_results)}ê°œ")
    return all_results



def build_vector_db_from_news(news_file=NEWS_JSON_FILE):
    loader = JSONLoader(file_path=news_file, jq_schema=".", text_content=False)
    documents = loader.load()
    print(f"ğŸ§  ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")

    vdb = VectorDB()
    for doc in documents:
        content = doc.metadata.get("content", "")
        if not content: continue
        vector = get_embedding(content)
        vdb.add_vector(vector, metadata=doc.metadata)

# ====================== ì‹¤í–‰ ===========================
if __name__ == "__main__":
    print("ğŸ“° BBC ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

    articles = fetch_bbc_news_from_to(
        query="",
        start_date="2025-03-01",  
        end_date="2025-03-27"
    )

    # âœ… ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(NEWS_JSON_FILE), exist_ok=True)

    # âœ… JSON ì €ì¥
    with open(NEWS_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=4)

    # âœ… ë²¡í„° DB êµ¬ì¶•
    build_vector_db_from_news()

    print("âœ… BBC ë‰´ìŠ¤ ìˆ˜ì§‘ + ì„ë² ë”© + ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")
