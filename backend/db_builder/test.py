import os
from langchain_weaviate.vectorstores import WeaviateVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from weaviate import WeaviateClient
from weaviate.connect import ConnectionParams

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path="/Users/hongminsik/Desktop/Capstone-project-2025/backend/.env")
print("âœ… API KEY:", os.getenv("OPENAI_API_KEY")[:8], "...")  # ì• 8ìë¦¬ë§Œ ì¶œë ¥

# âœ… Weaviate v4 ì—°ê²° ë° ì—°ê²° ì‹œì‘
connection_params = ConnectionParams.from_params(
    http_host="localhost",
    http_port=8080,
    http_secure=False,
    grpc_host="localhost",
    grpc_port=50051,
    grpc_secure=False
)
client = WeaviateClient(connection_params=connection_params)
client.connect()  # ğŸ”¥ ì—°ê²° í•„ìš”!

# âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vector_store = WeaviateVectorStore(
    client=client,
    index_name="news_bbc",
    embedding=embedding_model,
    text_key="text"  # âœ… ì‹¤ì œ Weaviate ì €ì¥ í•„ë“œëª…ìœ¼ë¡œ ìˆ˜ì •
)

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (LLM ê°€ì´ë“œ í¬í•¨)
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an expert in summarizing and answering questions based on news articles.

Here is the content retrieved from related news articles:
---------------------
{context}
---------------------

Based on the above information, answer the question.  
If the information is not found in the context, politely respond with "There is no relevant information in the articles."

Question: {question}
Answer:
"""
)

# âœ… RAG QA ì²´ì¸ êµ¬ì„±
retriever = vector_store.as_retriever(search_kwargs={"k": 6}) # 6ê°œì˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.5, # ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì ì¸ ë‹µë³€
    openai_api_key=os.getenv("OPENAI_API_KEY")
)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)

# âœ… í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
query = "please give me news about technology."
result = qa_chain.invoke(query)

print("\nğŸ§  Answer:")
print(result["result"])

print("\nğŸ“„ Source Documents:")
seen_urls = set()
for doc in result["source_documents"]:
    title = doc.metadata.get("title")
    url = doc.metadata.get("url")
    if url not in seen_urls:
        print("-", title, "|", url)
        seen_urls.add(url)


# âœ… Weaviate ì—°ê²° ì¢…ë£Œ
client.close()
print("\nğŸ”Œ Weaviate ì—°ê²° ì¢…ë£Œ")